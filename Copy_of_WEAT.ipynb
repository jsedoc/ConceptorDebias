{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of WEAT.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsedoc/ConceptorDebias/blob/ACL-cleanup/Copy_of_WEAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "l8Me4_OyFyKo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WEAT Algorithm\n",
        "## Test Statistic"
      ]
    },
    {
      "metadata": {
        "id": "mf6_liysF8en",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# returns s(w, A, B) for all w in W (passed as argument). Shape: n_words (in W) x 1\n",
        "def swAB(W, A, B):\n",
        "  #Calculate cosine-similarity between W and A, W and B\n",
        "  #print(\"W: \", W.shape, \" A: \", A.shape, \" B: \", B.shape)\n",
        "  WA = cosine_similarity(W,A)\n",
        "  WB = cosine_similarity(W,B)\n",
        "  #print('WA shape: ', WA.shape)\n",
        "  #Take mean along columns\n",
        "  WAmean = np.mean(WA, axis = 1)\n",
        "  WBmean = np.mean(WB, axis = 1)\n",
        "  \n",
        "  #print('sWAB shape: ', WAmean.shape)\n",
        "  \n",
        "  return (WAmean - WBmean)\n",
        "  \n",
        "def test_statistic(X, Y, A, B):\n",
        "  return (sum(swAB(X, A, B)) - sum(swAB(Y, A, B)))\n",
        "\n",
        "def weat_effect_size(X, Y, A, B, embd):\n",
        "  #Convert the set of words to matrix\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  # Find X U Y\n",
        "  XuY = list(set(X).union(Y))\n",
        "  XuYmat = []\n",
        "  for w in XuY:\n",
        "    if w.lower() in embd:\n",
        "      XuYmat.append(embd[w.lower()])\n",
        "  XuYmat = np.array(XuYmat)\n",
        "#   print('X U Y Shape: ', XuYmat.shape)\n",
        "#   print('X Shape: ', Xmat.shape)\n",
        "#   print('Y Shape: ', Ymat.shape)\n",
        "#   print('A Shape: ', Amat.shape)\n",
        "#   print('B Shape: ', Bmat.shape)\n",
        "  \n",
        "  d = (np.mean(swAB(Xmat,Amat,Bmat)) - np.mean(swAB(Ymat,Amat,Bmat)))/np.std(swAB(XuYmat, Amat, Bmat))\n",
        "  \n",
        "  return d"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "d8foGwVSGI16",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## P-Value"
      ]
    },
    {
      "metadata": {
        "id": "ZDy-duFOFj71",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "from itertools import combinations, filterfalse\n",
        "\n",
        "def random_permutation(iterable, r=None):\n",
        "  pool = tuple(iterable)\n",
        "  r = len(pool) if r is None else r\n",
        "  return tuple(random.sample(pool, r))\n",
        "\n",
        "def weat_p_value(X, Y, A, B, embd, sample):\n",
        "  size_of_permutation = min(len(X), len(Y))\n",
        "  X_Y = X + Y\n",
        "  test_stats_over_permutation = []\n",
        "  \n",
        "  Xmat = np.array([embd[w.lower()] for w in X if w.lower() in embd])\n",
        "  Ymat = np.array([embd[w.lower()] for w in Y if w.lower() in embd])\n",
        "  Amat = np.array([embd[w.lower()] for w in A if w.lower() in embd])\n",
        "  Bmat = np.array([embd[w.lower()] for w in B if w.lower() in embd])\n",
        "  \n",
        "  if not sample:\n",
        "      permutations = combinations(X_Y, size_of_permutation)\n",
        "  else:\n",
        "      permutations = [random_permutation(X_Y, size_of_permutation) for s in range(sample)]\n",
        "      \n",
        "  for Xi in permutations:\n",
        "    Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "    Ximat = np.array([embd[w.lower()] for w in Xi if w.lower() in embd])\n",
        "    Yimat = np.array([embd[w.lower()] for w in Yi if w.lower() in embd])\n",
        "    test_stats_over_permutation.append(test_statistic(Ximat, Yimat, Amat, Bmat))\n",
        "    \n",
        "  unperturbed = test_statistic(Xmat, Ymat, Amat, Bmat)\n",
        "  \n",
        "  is_over = np.array([o > unperturbed for o in test_stats_over_permutation])\n",
        "  #print(\"All: \", test_stats_over_permutation)\n",
        "  #print(\"Unpertrubed: \", unperturbed)\n",
        "  return is_over.sum() / is_over.size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NbRMmhwbGL98",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Test on sample input"
      ]
    },
    {
      "metadata": {
        "id": "upscdvtwFm0l",
        "colab_type": "code",
        "outputId": "c9706781-f440-4e9f-86a0-4e3a0ca7ef9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "X = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"] #Instruments\n",
        "Y = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"] #Weapons\n",
        "A = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"] #Pleasant\n",
        "B = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"] #Unpleasant\n",
        "\n",
        "#Load word embeddings\n",
        "#load gensim formatted Full Glove embeddings\n",
        "#!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')\n",
        "\n",
        "print('WEAT d = ', weat_effect_size(X, Y, A, B, glove))\n",
        "print('WEAT p = ', weat_p_value(X, Y, A, B, glove, 1000))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The glove embedding has been loaded!\n",
            "WEAT d =  1.5495627\n",
            "Unpertrubed:  2.2905553244054317\n",
            "WEAT p =  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rfwYNFl2ckGy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT with conceptor debiased embeddings"
      ]
    },
    {
      "metadata": {
        "id": "xEuq-Lt5ctV2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Compute the conceptor matrix for all words and gender specific words."
      ]
    },
    {
      "metadata": {
        "id": "HAG4oqwIc1Z3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Compute the conceptor matrix\n",
        "def process_cn_matrix(subspace, alpha = 2):\n",
        "  #print(\"starting...\")\n",
        "  #x = orig_embd.vectors\n",
        "  #print(subspace.shape)\n",
        "  \n",
        "  #Calculate the correlation matrix\n",
        "  R = subspace.dot(subspace.T)/(subspace.shape[1])\n",
        "  #print(\"R calculated\")\n",
        "  \n",
        "  #Calculate the conceptor matrix\n",
        "  C = R @ (np.linalg.inv(R + alpha ** (-2) * np.eye(subspace.shape[0])))\n",
        "  #print(\"C calculated\")\n",
        "  \n",
        "  #Calculate the negation of the conceptor matrix\n",
        "  negC = np.eye(subspace.shape[0]) - C\n",
        "  #print(\"negC calculated\")\n",
        "  \n",
        "  return negC\n",
        "\n",
        "def apply_conceptor(x, C):\n",
        "  #Post-process the vocab matrix\n",
        "  newX = (C @ x).T\n",
        "  print(newX.shape)\n",
        "  return newX"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GHaXxx5L3Ugb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Load embeddings of all words from the ref. wordlist from a specific embedding"
      ]
    },
    {
      "metadata": {
        "id": "8WBI1VED3bpM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Arguments - embd: The word embeddings in the form of a dict || wikiWordsPath: List of words to be considered\n",
        "def load_all_vectors(embd, wikiWordsPath):\n",
        "  all_words_index = {}\n",
        "  all_words_mat = []\n",
        "  with open(wikiWordsPath, \"r+\") as f_in:\n",
        "    ind = 0\n",
        "    for line in f_in:\n",
        "      word = line.split(' ')[0]\n",
        "      if word in embd:\n",
        "        all_words_index[word] = ind\n",
        "        all_words_mat.append(embd[word])\n",
        "        ind = ind+1\n",
        "        \n",
        "  return all_words_index, all_words_mat\n",
        "\n",
        "def load_subspace_vectors(embd, subspace_words):\n",
        "  subspace_embd_mat = []\n",
        "  ind = 0\n",
        "  for word in subspace_words:\n",
        "    if word in embd:\n",
        "      subspace_embd_mat.append(embd[word])\n",
        "      ind = ind+1\n",
        "      \n",
        "  return subspace_embd_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EzfTYgq0I9WW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load all word lists - Subspace"
      ]
    },
    {
      "metadata": {
        "id": "D4kndsi_DwNr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# General word list\n",
        "!wget https://raw.githubusercontent.com/IlyaSemenov/wikipedia-word-frequency/master/results/enwiki-20150602-words-frequency.txt\n",
        "!git clone https://github.com/PrincetonML/SIF\n",
        "    \n",
        "# Gender word lists\n",
        "!git clone https://github.com/uclanlp/gn_glove\n",
        "!git clone https://github.com/uclanlp/corefBias\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/female.txt\n",
        "!wget https://www.cs.cmu.edu/Groups/AI/areas/nlp/corpora/names/male.txt\n",
        "    \n",
        "# our code for debiasing -- also includes word lists    \n",
        "!git clone https://github.com/jsedoc/ConceptorDebias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "orBvfpqCFOfa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from load_word_lists import *\n",
        "\n",
        "gender_list_pronouns = WEATLists.W_7_Male_terms + WEATLists.W_7_Female_terms + WEATLists.W_8_Male_terms + WEATLists.W_8_Female_terms\n",
        "gender_list_pronouns = list(set(gender_list_pronouns))\n",
        "\n",
        "gender_list_extended = male_vino_extra + female_vino_extra + male_gnGlove + female_gnGlove\n",
        "gender_list_extended = list(set(gender_list_extended))\n",
        "\n",
        "gender_list_propernouns = male_cmu + female_cmu\n",
        "gender_list_propernouns = list(set(gender_list_propernouns))\n",
        "\n",
        "gender_list_all = gender_list_pronouns + gender_list_extended + gender_list_propernouns\n",
        "gender_list_all = list(set(gender_list_all))\n",
        "\n",
        "race_list = WEATLists.W_3_Unused_full_list_European_American_names + WEATLists.W_3_European_American_names + WEATLists.W_3_Unused_full_list_African_American_names + WEATLists.W_3_African_American_names + WEATLists.W_4_Unused_full_list_European_American_names + WEATLists.W_4_European_American_names + WEATLists.W_4_Unused_full_list_African_American_names + WEATLists.W_4_African_American_names + WEATLists.W_5_Unused_full_list_European_American_names + WEATLists.W_5_European_American_names + WEATLists.W_5_Unused_full_list_African_American_names + WEATLists.W_5_African_American_names \n",
        "race_list = list(set(race_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kEiBEVI2I2WR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load different embeddings"
      ]
    },
    {
      "metadata": {
        "id": "Nrlqu2xvJH_6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Glove"
      ]
    },
    {
      "metadata": {
        "id": "PFNSjmY8I1Ut",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Load word embeddings\n",
        "#download gensim formatted Full Glove embeddings\n",
        "!gdown https://drive.google.com/uc?id=1Ty2exMyi-XOufY-v81RJfiPvnintHuy2\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "glove = KeyedVectors.load_word2vec_format(resourceFile + 'gensim_glove.840B.300d.txt.bin', binary=True)\n",
        "print('The glove embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aD-bmiCdGKYt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "print(X)\n",
        "a = [glove[w] for w in X if w.lower() in glove]\n",
        "print(np.array(a).shape)\n",
        "glove['Brad']\n",
        "#glove['brad']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "84af1zflJKLQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "OmJlU2IiJMMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#load gensim formatted Full Word2vec embeddings\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz\n",
        "  \n",
        "import gensim\n",
        "\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "resourceFile = ''\n",
        "\n",
        "word2vec = KeyedVectors.load_word2vec_format(resourceFile + 'GoogleNews-vectors-negative300.bin', binary=True)\n",
        "print('The word2vec embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FCiZfu13JPHl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Fasttext"
      ]
    },
    {
      "metadata": {
        "id": "A9zwQ2T_JRBo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1Zl6a75Ybf8do9uupmrJWKQMnvqqme4fh\n",
        "\n",
        "import gensim\n",
        "from gensim.models.keyedvectors import KeyedVectors  \n",
        "\n",
        "resourceFile = ''\n",
        "fasttext = KeyedVectors.load_word2vec_format(resourceFile + 'fasttext.bin', binary=True)\n",
        "print('The fasttext embedding has been loaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7hL2zgX6JRhG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Elmo"
      ]
    },
    {
      "metadata": {
        "id": "cXvK1wuvJTVP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=17TK2h3cz7amgm2mCY4QCYy1yh23ZFWDU\n",
        "  \n",
        "import pickle\n",
        "data = pickle.load(open(\"elmo_embeddings_emma_brown.pkl\", \"rb\"))\n",
        "\n",
        "def pick_embeddings(corpus, sent_embs):\n",
        "    X = []\n",
        "    labels = {}\n",
        "    sents = []\n",
        "    ind = 0\n",
        "    for i, s in enumerate(corpus):\n",
        "        for j, w in enumerate(s):\n",
        "            X.append(sent_embs[i][j])\n",
        "            if w.lower() in labels:\n",
        "              labels[w.lower()].append(ind)\n",
        "            else:\n",
        "              labels[w.lower()] = [ind]\n",
        "            sents.append(s)\n",
        "            ind = ind + 1\n",
        "    return (X, labels, sents)\n",
        "  \n",
        "def get_word_list(path):\n",
        "    word_list = []\n",
        "    with open(path, \"r+\") as f_in:\n",
        "      for line in f_in:\n",
        "        word = line.split(' ')[0]\n",
        "        word_list.append(word.lower())\n",
        "\n",
        "    return word_list\n",
        "\n",
        "def load_subspace_vectors_contextual(all_mat, all_index, subspace_list):\n",
        "    subspace_mat = []\n",
        "    for w in subspace_list:\n",
        "      if w.lower() in all_index:\n",
        "        for i in all_index[w.lower()]:\n",
        "          #print(type(i))\n",
        "          subspace_mat.append(all_mat[i])\n",
        "    #subspace_mat = [all_mat[i,:] for i in all_index[w.lower()] for w in subspace_list if w.lower() in all_index]\n",
        "    print(\"Subspace: \", np.array(subspace_mat).shape)\n",
        "    return subspace_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SZy2oUe0nlxz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "#nltk.download('brown')\n",
        "\n",
        "brown_corpus = brown.sents()\n",
        "elmo = data['brown_embs']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gTsLdzKKrnn6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(np.array(elmo_brown_mat).shape)\n",
        "print(len(list(elmo_brown_index.keys())))\n",
        "print(len(wiki_words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L541j-yEJWa4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Post-process (CN) all embeddings using a particular subspace\n",
        "\n",
        "Test WEAT for a given pair of attribute and target pairs.\n",
        "\n",
        "Note: To change the attribute and target word sets, change the values of X, Y, A, B. (It should be a list of words)\n",
        "\n",
        "Subspace(s) to be tested upon can be changed by setting 'all_subspace' to a list of variables representing the subspace. Each of these variables should be a list of words for that subspace."
      ]
    },
    {
      "metadata": {
        "id": "GeoHVWtg8jkn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "#List of all embeddings to test on\n",
        "all_embd = ['glove', 'word2vec', 'fasttext','elmo']\n",
        "\n",
        "#List of all subspaces for gender\n",
        "all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "\n",
        "#List of all subsoaces for race\n",
        "# all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "# X = WEATLists.W_6_Career\n",
        "# Y = WEATLists.W_6_Family\n",
        "# A = WEATLists.W_6_Male_names\n",
        "# B = WEATLists.W_6_Female_names\n",
        "\n",
        "X = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "Y = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "A = WEATLists.W_5_Pleasant\n",
        "B = WEATLists.W_5_Unpleasant\n",
        "\n",
        "# X = WEATLists.W_8_Science\n",
        "# Y = WEATLists.W_8_Arts\n",
        "# A = WEATLists.W_8_Male_terms\n",
        "# B = WEATLists.W_8_Female_terms\n",
        "\n",
        "\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    #wiki_words = get_word_list('SIF/auxiliary_data/enwiki_vocab_min200.txt')\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_conceptor':\n",
        "      #CN all word embeddings using the respective subspace\n",
        "      if subspace == 'gender_list_and':\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat1 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T, alpha = 8)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T, alpha = 3)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors_contextual(all_words_mat, all_words_index, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T, alpha = 10)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "        else:\n",
        "          subspace_words_mat1 = load_subspace_vectors(curr_embd, gender_list_pronouns)\n",
        "          cn1 = process_cn_matrix(np.array(subspace_words_mat1).T)\n",
        "\n",
        "          subspace_words_mat2 = load_subspace_vectors(curr_embd, gender_list_extended)\n",
        "          cn2 = process_cn_matrix(np.array(subspace_words_mat2).T)\n",
        "\n",
        "          subspace_words_mat3 = load_subspace_vectors(curr_embd, gender_list_propernouns)\n",
        "          cn3 = process_cn_matrix(np.array(subspace_words_mat3).T)\n",
        "\n",
        "          cn = AND(cn1, AND(cn2, cn3))\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      else:\n",
        "        #Load all embeddings of the subspace as a matrix\n",
        "        if embd == 'elmo' or embd == 'bert':\n",
        "          subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T, alpha = 6)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "          print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "        else:\n",
        "          subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "          cn = process_cn_matrix(np.array(subspace_words_mat).T)\n",
        "          all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "#     print(\"LAST: \", np.array(all_words[\"a\"]).shape)\n",
        "    if subspace == 'without_conceptor':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(X, Y, A, B, all_words)\n",
        "      p_cn = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "      print(\"Without conceptor\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(X, Y, A, B, all_words)\n",
        "      p_cn = weat_p_value(X, Y, A, B, all_words, 1000)\n",
        "      print(\"With conceptor: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yAdi5tlZGRzL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT algorithm from GITHUB gist\n",
        "Ref: https://gist.github.com/SandyRogers/e5c2e938502a75dcae25216e4fae2da5"
      ]
    },
    {
      "metadata": {
        "id": "2YM0FV9SFrWC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class WEATTest(object):\n",
        "    \"\"\"\n",
        "    Perform WEAT (Word Embedding Association Test) bias tests on a language model.\n",
        "    Follows from Caliskan et al 2017 (10.1126/science.aal4230).\n",
        "    \"\"\"\n",
        "    \n",
        "    instruments = [\"bagpipe\", \"cello\", \"guitar\", \"lute\", \"trombone\", \"banjo\", \"clarinet\", \"harmonica\", \"mandolin\", \"trumpet\", \"bassoon\", \"drum\", \"harp\", \"oboe\", \"tuba\", \"bell\", \"fiddle\", \"harpsichord\", \"piano\", \"viola\", \"bongo\",\n",
        "\"flute\", \"horn\", \"saxophone\", \"violin\"]\n",
        "    weapons = [\"arrow\", \"club\", \"gun\", \"missile\", \"spear\", \"axe\", \"dagger\", \"harpoon\", \"pistol\", \"sword\", \"blade\", \"dynamite\", \"hatchet\", \"rifle\", \"tank\", \"bomb\", \"firearm\", \"knife\", \"shotgun\", \"teargas\", \"cannon\", \"grenade\",\n",
        "    \"mace\", \"slingshot\", \"whip\"]\n",
        "    flowers = [\"aster\", \"clover\", \"hyacinth\", \"marigold\", \"poppy\", \"azalea\", \"crocus\", \"iris\", \"orchid\", \"rose\", \"blue-bell\", \"daffodil\", \"lilac\", \"pansy\", \"tulip\", \"buttercup\", \"daisy\", \"lily\", \"peony\", \"violet\", \"carnation\", \"gladiola\", \"magnolia\", \"petunia\", \"zinnia\"]\n",
        "    insects = [\"ant\", \"caterpillar\", \"flea\", \"locust\", \"spider\", \"bedbug\", \"centipede\", \"fly\", \"maggot\", \"tarantula\", \"bee\", \"cockroach\", \"gnat\", \"mosquito\", \"termite\", \"beetle\", \"cricket\", \"hornet\", \"moth\", \"wasp\", \"blackfly\",\n",
        "    \"dragonfly\", \"horsefly\", \"roach\", \"weevil\"]\n",
        "    pleasant = [\"caress\", \"freedom\", \"health\", \"love\", \"peace\", \"cheer\", \"friend\", \"heaven\", \"loyal\", \"pleasure\", \"diamond\", \"gentle\", \"honest\", \"lucky\", \"rainbow\", \"diploma\", \"gift\", \"honor\", \"miracle\", \"sunrise\", \"family\",\n",
        "    \"happy\", \"laughter\", \"paradise\", \"vacation\"]\n",
        "    unpleasant = [\"abuse\", \"crash\", \"filth\", \"murder\", \"sickness\", \"accident\", \"death\", \"grief\", \"poison\", \"stink\", \"assault\", \"disaster\", \"hatred\", \"pollute\", \"tragedy\", \"divorce\", \"jail\", \"poverty\", \"ugly\", \"cancer\", \"kill\", \"rotten\",\n",
        "    \"vomit\", \"agony\", \"prison\"]\n",
        "    math = [\"math\", \"algebra\", \"geometry\", \"calculus\", \"equations\", \"computation\", \"numbers\", \"addition\"]\n",
        "    arts = [\"poetry\", \"art\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\", \"sculpture\"]\n",
        "    male = [\"male\", \"man\", \"boy\", \"brother\", \"he\", \"him\", \"his\", \"son\"]\n",
        "    female = [\"female\", \"woman\", \"girl\", \"sister\", \"she\", \"her\", \"hers\", \"daughter\"]\n",
        "    science = [\"science\", \"technology\", \"physics\", \"chemistry\", \"Einstein\", \"NASA\", \"experiment\", \"astronomy\"]\n",
        "    arts_2 = [\"poetry\", \"art\", \"Shakespeare\", \"dance\", \"literature\", \"novel\", \"symphony\", \"drama\"]\n",
        "    male_2 = [\"brother\", \"father\", \"uncle\", \"grandfather\", \"son\", \"he\", \"his\", \"him\"]\n",
        "    female_2 = [\"sister\", \"mother\", \"aunt\", \"grandmother\", \"daughter\", \"she\", \"hers\", \"he\"]\n",
        "    mental_disease = [\"sad\", \"hopeless\", \"gloomy\", \"tearful\", \"miserable\", \"depressed\"]\n",
        "    physical_disease = [\"sick\", \"illness\", \"influenza\", \"disease\", \"virus\", \"cancer\"]\n",
        "    temporary = [\"impermanent\", \"unstable\", \"variable\", \"fleeting\", \"short-term\", \"brief\", \"occasional\"]\n",
        "    permanent = [\"stable\", \"always\", \"constant\", \"persistent\", \"chronic\", \"prolonged\", \"forever\"]\n",
        "    \n",
        "    def __init__(self, model):\n",
        "        \"\"\"Setup a Word Embedding Association Test for a given spaCy language model.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> nlp = spacy.load('en_core_web_md')\n",
        "            >>> test = WEATTest(nlp)\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "\n",
        "    @staticmethod\n",
        "    def word_association_with_attribute(self, w, A, B):\n",
        "        return np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(a).reshape(1,-1)) for a in A]) - np.mean([cosine_similarity(np.array(w).reshape(1,-1),np.array(b).reshape(1,-1)) for b in B])\n",
        "\n",
        "    @staticmethod\n",
        "    def differential_assoication(self, X, Y, A, B):\n",
        "        return np.sum([self.word_association_with_attribute(self, x, A, B) for x in X]) - np.sum([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_effect_size(self, X, Y, A, B):\n",
        "        return (\n",
        "            np.mean([self.word_association_with_attribute(self, x, A, B) for x in X]) -\n",
        "            np.mean([self.word_association_with_attribute(self, y, A, B) for y in Y])\n",
        "        ) / np.std([self.word_association_with_attribute(self, w, A, B) for w in X + Y])\n",
        "\n",
        "    @staticmethod\n",
        "    def random_permutation(self, iterable, r=None):\n",
        "        pool = tuple(iterable)\n",
        "        r = len(pool) if r is None else r\n",
        "        return tuple(random.sample(pool, r))\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_p_value(self, X, Y, A, B, sample):\n",
        "        size_of_permutation = min(len(X), len(Y))\n",
        "        X_Y = X + Y\n",
        "        observed_test_stats_over_permutations = []\n",
        "\n",
        "        if not sample:\n",
        "            permutations = combinations(X_Y, size_of_permutation)\n",
        "        else:\n",
        "            permutations = [self.random_permutation(self, X_Y, size_of_permutation) for s in range(sample)]\n",
        "        print(np.array(X_Y).shape)\n",
        "        for Xi in permutations:\n",
        "            Yi = filterfalse(lambda w:w in Xi, X_Y)\n",
        "            observed_test_stats_over_permutations.append(self.differential_assoication(self, Xi, Yi, A, B))\n",
        "\n",
        "        unperturbed = self.differential_assoication(self, X, Y, A, B)\n",
        "        is_over = np.array([o > unperturbed for o in observed_test_stats_over_permutations])\n",
        "        return is_over.sum() / is_over.size\n",
        "\n",
        "    @staticmethod\n",
        "    def weat_stats(X, Y, A, B, self, sample_p=None):\n",
        "        test_statistic = self.differential_assoication(self, X, Y, A, B)\n",
        "        effect_size = self.weat_effect_size(self, X, Y, A, B)\n",
        "        p = self.weat_p_value(self, X, Y, A, B, sample=sample_p)\n",
        "        return test_statistic, effect_size, p\n",
        "\n",
        "    def run_test(self, target_1, target_2, attributes_1, attributes_2, sample_p=None):\n",
        "        \"\"\"Run the WEAT test for differential association between two \n",
        "        sets of target words and two seats of attributes.\n",
        "        \n",
        "        EXAMPLE:\n",
        "            >>> test.run_test(WEATTest.instruments, WEATTest.weapon, WEATTest.pleasant, WEATTest.unpleasant)\n",
        "            >>> test.run_test(a, b, c, d, sample_p=1000) # use 1000 permutations for p-value calculation\n",
        "            >>> test.run_test(a, b, c, d, sample_p=None) # use all possible permutations for p-value calculation\n",
        "            \n",
        "        RETURNS:\n",
        "            (d, e, p). A tuple of floats, where d is the WEAT Test statistic, \n",
        "            e is the effect size, and p is the one-sided p-value measuring the\n",
        "            (un)likeliness of the null hypothesis (which is that there is no\n",
        "            difference in association between the two target word sets and\n",
        "            the attributes).\n",
        "            \n",
        "            If e is large and p small, then differences in the model between \n",
        "            the attribute word sets match differences between the targets.\n",
        "        \"\"\"\n",
        "        X = [list(self.model[w]) for w in target_1]\n",
        "        Y = [list(self.model[w]) for w in target_2]\n",
        "        A = [list(self.model[w]) for w in attributes_1]\n",
        "        B = [list(self.model[w]) for w in attributes_2]\n",
        "        print(X)\n",
        "        return self.weat_stats(X, Y, A, B, self, sample_p)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2I9dGN5fGQw-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## WEAT on BERT embeddings"
      ]
    },
    {
      "metadata": {
        "id": "hDL0wCtSwoOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "#Load all bert embeddings in a dictonary\n",
        "all = {}\n",
        "for filename in os.listdir('/home/saketk/bert'):\n",
        "  all[filename] = pickle.load(open(filename, \"rb\"))\n",
        "\n",
        "#Load all embeddings in a matrix and a dictonary of words to row numbers\n",
        "res = all['big_bert_gender_list_extended.pkl']['type_embedings']\n",
        "w = []\n",
        "for name in all:\n",
        "  res = np.concatenate((res, all[name]['type_embedings']))\n",
        "  w += all[name]['words']\n",
        "print(res.shape)\n",
        "print(len(set(w)))\n",
        "w = [aaa.lower() for aaa in w]\n",
        "\n",
        "#Load all BERT conceptors\n",
        "cn_pronouns = all['big_bert_gender_list_pronouns.pkl']['GnegC']\n",
        "cn_propernouns = all['big_bert_gender_list_propernouns.pkl']['GnegC']\n",
        "cn_extended = all['big_bert_gender_list_extended.pkl']['GnegC']\n",
        "cn_all = all['big_bert_gender_list_all.pkl']['GnegC']\n",
        "cn_race = all['big_bert_race_list.pkl']['GnegC']\n",
        "\n",
        "print(np.array(cn_all).shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JZ5hwHPey84G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test Debiasing on BERT Embeddings using WEAT"
      ]
    },
    {
      "metadata": {
        "id": "cWeJf7sDkSCc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from ConceptorDebias.conceptor_fxns import AND\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['bert']\n",
        "all_subspace = ['without_conceptor','gender_list_pronouns', 'gender_list_extended','gender_list_propernouns', 'gender_list_all', 'gender_list_and']\n",
        "# all_subspace = ['without_conceptor', 'race_list']\n",
        "\n",
        "all_words_mat = res\n",
        "all_words_index = {}\n",
        "print(len(set(w)))\n",
        "for i,a in enumerate(w):\n",
        "  all_words_index[a] = i\n",
        "# all_words_index = {wr:i for i,wr in enumerate(w)}\n",
        "print(\"All_words_mat: \", all_words_mat.shape)\n",
        "print(\"Index: \", len(all_words_index.keys()))\n",
        "career = WEATLists.W_8_Science\n",
        "family = WEATLists.W_8_Arts\n",
        "male = WEATLists.W_8_Male_terms\n",
        "female = WEATLists.W_8_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "embd = 'bert'\n",
        "results = []\n",
        "for subspace in all_subspace:\n",
        "    \n",
        "  if subspace != 'without_conceptor' and subspace != 'gender_list_and':\n",
        "    subspace_words_list = eval(subspace)\n",
        "\n",
        "  if subspace != 'without_conceptor':\n",
        "    #CN all word embeddings using the respective subspace\n",
        "    if subspace == 'gender_list_and':\n",
        "      cn = AND(cn_pronouns, AND(cn_extended, cn_propernouns))\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"All mat CN: \", np.array(all_words_cn).shape)\n",
        "    else:\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if subspace == 'gender_list_pronouns':\n",
        "        cn = cn_pronouns\n",
        "      elif subspace == 'gender_list_propernouns':\n",
        "        cn = cn_propernouns\n",
        "      elif subspace == 'gender_list_extended':\n",
        "        cn = cn_extended\n",
        "      elif subspace == 'gender_list_all':\n",
        "        cn = cn_all\n",
        "        \n",
        "      print(\"CN shape: \", np.array(cn).shape)\n",
        "      all_words_cn = apply_conceptor(np.array(all_words_mat).T, np.array(cn))\n",
        "      print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      \n",
        "  else:\n",
        "    all_words_cn = all_words_mat\n",
        "  all_words_cn = np.array(all_words_cn)\n",
        "  print(\"All CN: \", all_words_cn.shape)\n",
        "  #Store all conceptored words in a dictonary\n",
        "  all_words = {}\n",
        "  for word, index in all_words_index.items():\n",
        "    all_words[word] = all_words_cn[index,:]\n",
        "  print(\"D: \", len(all_words.keys()))\n",
        "  if subspace == 'without_conceptor':\n",
        "    #WITHOUT CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"Without conceptor\")\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "  else:\n",
        "    #WITH CONCEPTOR\n",
        "    d_cn = weat_effect_size(career, family, male, female, all_words)\n",
        "    p_cn = weat_p_value(career, family, male, female, all_words, 1000)\n",
        "    print(\"With conceptor: \", embd, subspace)\n",
        "    print('WEAT d = ', d_cn)\n",
        "    print('WEAT p = ', p_cn)\n",
        "\n",
        "  row = [embd, subspace, d_cn, p_cn]\n",
        "  results.append(row)\n",
        "\n",
        "  \n",
        "  \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Gbg0wsR_Ykym",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "bEzUV_AOtRAd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mu et. al. Hard Debiasing"
      ]
    },
    {
      "metadata": {
        "id": "a0ZvHFuFtV_v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "#Project off first principal component\n",
        "def hard_debias(all_words, subspace):\n",
        "  all_words = np.array(all_words)\n",
        "  subspace = np.array(subspace)\n",
        "  print(all_words.shape, \" \", subspace.shape)\n",
        "  pca = PCA(n_components = 1)\n",
        "  pca.fit(subspace)\n",
        "  pc1 = np.array(pca.components_)\n",
        "  \n",
        "  temp = (pc1.T @ (pc1 @ all_words.T)).T\n",
        "  ret = all_words - temp\n",
        "  \n",
        "  return ret\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9t0IHhGhznxG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test Hard Debiasing using WEAT"
      ]
    },
    {
      "metadata": {
        "id": "ESv2ZU7r1PV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext','elmo']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "# all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "# science = WEATLists.W_8_Science\n",
        "# arts = WEATLists.W_8_Arts\n",
        "# male = WEATLists.W_8_Male_terms\n",
        "# female = WEATLists.W_8_Female_terms\n",
        "\n",
        "white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "pleasant = WEATLists.W_5_Pleasant\n",
        "unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = hard_debias(all_words_mat, subspace_words_mat)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(white, black, pleasant, unpleasant, all_words)\n",
        "      p_cn = weat_p_value(white, black, pleasant, unpleasant, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iP9IM02Ln-qp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Bolukbasi hard debiasing"
      ]
    },
    {
      "metadata": {
        "id": "N-5CfftKoGCt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def doPCA(pairs, mat, index, num_components = 5):\n",
        "    matrix = []\n",
        "    for a, b in pairs:\n",
        "        center = (mat[index[a.lower()]] + mat[index[b.lower()]])/2\n",
        "        matrix.append(mat[index[a.lower()]] - center)\n",
        "        matrix.append(mat[index[b.lower()]] - center)\n",
        "    matrix = np.array(matrix)\n",
        "    pca = PCA(n_components = num_components)\n",
        "    pca.fit(matrix)\n",
        "    # bar(range(num_components), pca.explained_variance_ratio_)\n",
        "    return pca\n",
        "\n",
        "def drop(u, v):\n",
        "    return u - v * u.dot(v) / v.dot(v)\n",
        "  \n",
        "def normalize(all_words_mat):\n",
        "    all_words_mat /= np.linalg.norm(all_words_mat, axis=1)[:, np.newaxis]\n",
        "    return all_words_mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GWOwX9wHoB-7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def debias(all_words_mat, all_words_index, gender_specific_words, definitional, equalize):\n",
        "    gender_direction = doPCA(definitional, all_words_mat, all_words_index).components_[0]\n",
        "    specific_set = set(gender_specific_words)\n",
        "    for w in list(all_words_index.keys()):\n",
        "        if w not in specific_set:\n",
        "            all_words_mat[all_words_index[w.lower()]] = drop(all_words_mat[all_words_index[w.lower()]], gender_direction)\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    candidates = {x for e1, e2 in equalize for x in [(e1.lower(), e2.lower()),\n",
        "                                                     (e1.title(), e2.title()),\n",
        "                                                     (e1.upper(), e2.upper())]}\n",
        "    print(candidates)\n",
        "    for (a, b) in candidates:\n",
        "        if (a.lower() in all_words_index and b.lower() in all_words_index):\n",
        "            y = drop((all_words_mat[all_words_index[a.lower()]] + all_words_mat[all_words_index[b.lower()]]) / 2, gender_direction)\n",
        "            z = np.sqrt(1 - np.linalg.norm(y)**2)\n",
        "            if (all_words_mat[all_words_index[a.lower()]] - all_words_mat[all_words_index[b.lower()]]).dot(gender_direction) < 0:\n",
        "                z = -z\n",
        "            all_words_mat[all_words_index[a.lower()]] = z * gender_direction + y\n",
        "            all_words_mat[all_words_index[b.lower()]] = -z * gender_direction + y\n",
        "    all_words_mat = normalize(all_words_mat)\n",
        "    return all_words_mat\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t9h4VxZfqluU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "resourceFile = ''\n",
        "wikiWordsPath = resourceFile + 'SIF/auxiliary_data/enwiki_vocab_min200.txt' # https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt\n",
        "\n",
        "%cd debiaswe_tutorial/debiaswe/\n",
        "# Lets load some gender related word lists to help us with debiasing\n",
        "with open('./data/definitional_pairs.json', \"r\") as f:\n",
        "    defs = json.load(f) #gender definitional words\n",
        "print(\"definitional\", defs)\n",
        "defs_list = []\n",
        "for pair in defs:\n",
        "  defs_list.append(pair[0])\n",
        "  defs_list.append(pair[1])\n",
        "\n",
        "with open('./data/equalize_pairs.json', \"r\") as f:\n",
        "    equalize_pairs = json.load(f) \n",
        "print(\"Equalize pairs\", equalize_pairs)\n",
        "\n",
        "%cd ../../\n",
        "!ls\n",
        "\n",
        "all_embd = ['glove', 'word2vec', 'fasttext']\n",
        "all_subspace = ['without_debiasing', 'gender_list_extended','gender_list_propernouns','gender_list_pronouns', 'gender_list_all']\n",
        "#all_subspace = ['without_debiasing', 'race_list']\n",
        "\n",
        "math = WEATLists.W_7_Math\n",
        "arts = WEATLists.W_7_Arts\n",
        "male = WEATLists.W_7_Male_terms\n",
        "female = WEATLists.W_7_Female_terms\n",
        "\n",
        "# white = WEATLists.W_5_Unused_full_list_European_American_names\n",
        "# black = WEATLists.W_5_Unused_full_list_African_American_names\n",
        "# pleasant = WEATLists.W_5_Pleasant\n",
        "# unpleasant = WEATLists.W_5_Unpleasant\n",
        "\n",
        "#print(career, family, male, female)\n",
        "results = []\n",
        "\n",
        "for embd in all_embd:\n",
        "  #Initialize the embeddings to be used\n",
        "  curr_embd = eval(embd)\n",
        "  \n",
        "  #Load all embeddings in a matrix of all words in the wordlist\n",
        "  if embd == 'elmo' or embd == 'bert':\n",
        "    all_words_mat, all_words_index, _ = pick_embeddings(brown_corpus, curr_embd)\n",
        "    print(\"All mat: \", np.array(all_words_mat).shape)\n",
        "    print(\"Number of words: \", len(list(all_words_index.keys())))\n",
        "  else:\n",
        "    all_words_index, all_words_mat = load_all_vectors(curr_embd, wikiWordsPath)\n",
        "  \n",
        "  for subspace in all_subspace:\n",
        "    \n",
        "    if subspace != 'without_debiasing' and subspace != 'gender_list_and':\n",
        "      subspace_words_list = eval(subspace)\n",
        "\n",
        "    \n",
        "    if subspace != 'without_debiasing':\n",
        "      #Load all embeddings of the subspace as a matrix\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        #subspace_words_mat = load_subspace_vectors_contextual(all_words_mat, all_words_index, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "        print(\"Subspace mat: \", np.array(subspace_words_mat).shape)\n",
        "      else:\n",
        "        #subspace_words_mat = load_subspace_vectors(curr_embd, subspace_words_list)\n",
        "        all_words_cn = debias(all_words_mat, all_words_index, subspace_words_list, defs, equalize_pairs)\n",
        "    else:\n",
        "      all_words_cn = all_words_mat\n",
        "    all_words_cn = np.array(all_words_cn)\n",
        "    print(\"All CN: \", all_words_cn.shape)\n",
        "    #Store all conceptored words in a dictonary\n",
        "    all_words = {}\n",
        "    for word, index in all_words_index.items():\n",
        "      #print(word, index)\n",
        "      if embd == 'elmo' or embd == 'bert':\n",
        "        all_words[word] = np.mean([all_words_cn[i,:] for i in index], axis = 0)\n",
        "      else:\n",
        "        all_words[word] = all_words_cn[index,:]\n",
        "    \n",
        "    if subspace == 'without_debiasing':\n",
        "      #WITHOUT CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"Without debiasing\")\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    else:\n",
        "      #WITH CONCEPTOR\n",
        "      d_cn = weat_effect_size(math, arts, male, female, all_words)\n",
        "      p_cn = weat_p_value(math, arts, male, female, all_words, 1000)\n",
        "      print(\"With debiasing: \", embd, subspace)\n",
        "      print('WEAT d = ', d_cn)\n",
        "      print('WEAT p = ', p_cn)\n",
        "    \n",
        "    row = [embd, subspace, d_cn, p_cn]\n",
        "    results.append(row)\n",
        "    \n",
        "pd.DataFrame(np.array(results), columns = ['Embedding', 'Subspace', 'Effect Size', 'p-value'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6Xi-W-t1DZC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}